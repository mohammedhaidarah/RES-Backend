# -*- coding: utf-8 -*-
"""RES.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pilL-yA8I_EpQiuUMDdt-gHFJKOV6NsV

#Unedited KNN Version
###Mit Data Visualization, etc.
"""
from csv import writer

from careerjet_api import CareerjetAPIClient
import pandas as pd
#import numpy as np
#import matplotlib.pyplot as plt
import warnings
#from sklearn.naive_bayes import MultinomialNB
from sklearn.multiclass import OneVsRestClassifier
from sklearn import metrics
from sklearn.metrics import accuracy_score
#from pandas.plotting import scatter_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
#from pathlib import Path
import nltk
from nltk.corpus import stopwords
import string
#from wordcloud import WordCloud
#import seaborn as sns
#from matplotlib.gridspec import GridSpec
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
import PyPDF2 
import json
from sklearn.preprocessing import LabelEncoder
#from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
#from sklearn.metrics import confusion_matrix
#warnings.filterwarnings('ignore')


from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

resumeDataSet = pd.read_csv('UpdatedResumeDataSet.csv' ,encoding='utf-8')
#resumeDataSet = pd.read_csv('LinkedIn_ProfileDataset_Text_Role_200.csv' ,encoding='utf-8')
row_count = sum(1 for row in resumeDataSet)
numbers_of_rows = 1500
"""####Functions"""

def cleanString(s):
  s = re.sub('http\S+\s*', ' ', s)  # remove URLs
  s = re.sub('RT|cc', ' ', s)  # remove RT and cc
  s = re.sub('#\S+', '', s)  # remove hashtags
  s = re.sub('@\S+', '  ', s)  # remove mentions
  s = re.sub('[%s]' % re.escape("""!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"""), ' ', s)  # remove punctuations
  s = re.sub(r'[^\x00-\x7f]',r' ', s) 
  s = re.sub('\s+', ' ', s)  # remove extra whitespace
  return s
  
def vectorize(requiredText,):
  word_vectorizer = TfidfVectorizer(
    sublinear_tf=True,
    stop_words='english',
    max_features=numbers_of_rows)
  word_vectorizer.fit(requiredText)
  WordFeatures = word_vectorizer.transform(requiredText)
  return WordFeatures

def cleanResume(resumeText):
    resumeText = re.sub('http\S+\s*', ' ', resumeText)  # remove URLs
    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc
    resumeText = re.sub('#\S+', '', resumeText)  # remove hashtags
    resumeText = re.sub('@\S+', '  ', resumeText)  # remove mentions
    resumeText = re.sub('[%s]' % re.escape("""!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"""), ' ', resumeText)  # remove punctuations
    resumeText = re.sub(r'[^\x00-\x7f]',r' ', resumeText) 
    resumeText = re.sub('\s+', ' ', resumeText)  # remove extra whitespace
    return resumeText

def pdf_to_json(pdffile,jsonfile="testResume.json"):
        # Opening JSON file
    f = open(jsonfile)
    # Open pdf file
    pdfFileObj = open(pdffile,'rb')

        # Read file
    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)

        # Get total number of pages
    num_pages = pdfReader.numPages

        # Initialize a count for the number of pages
    count = 0

        # Initialize a text empty string variable
    text = ""

        # Extract text from every page on the file
    while count < num_pages:
        pageObj = pdfReader.getPage(count)
        count +=1
        text += pageObj.extractText()
            
        # Convert all strings to lowercase
    text = text.lower()

        # Remove numbers
    text = re.sub(r'\d+','',text)

        # Remove punctuation
    #text = text.translate(str.maketrans('','',string.punctuation))  
    #text = text.replace('\n','')
    text =  cleanResume(text)
    # returns JSON object as 
    # a dictionary
    data = json.load(f)
    
    # Iterating through the json
    # list
    ab = text.find("skills")
    data[0]['Resume'] = text[ab:]
    #data[0]['Resume'] = text[100:]

    with open(jsonfile, 'w') as f:
        json.dump(data, f)
    
    # Closing file
    f.close()
    return text


def run(jsonfile="testResume.json"):   

    global resumeDataSet
    global numbers_of_rows
    #resumeDataSet = pd.read_json('dataset.json',lines=True)
    #resumeDataSet = pd.read_csv('resume_dataset.csv' ,encoding='utf-8')
    
    resumeDataSet['cleaned_resume'] = ''
    resumeDataSet.head()


    #targetCounts = resumeDataSet['Category'].value_counts()
    #targetLabels  = resumeDataSet['Category'].unique()
    # Make square figures and axes
    #plt.figure(1, figsize=(25,25))
    #the_grid = GridSpec(2, 2)


    #cmap = plt.get_cmap('coolwarm')
    #colors = [cmap(i) for i in np.linspace(0, 1, 3)]
    #plt.subplot(the_grid[0, 1], aspect=1, title='CATEGORY DISTRIBUTION')

    #source_pie = plt.pie(targetCounts, labels=targetLabels, autopct='%1.1f%%', shadow=True, colors=colors)
    #plt.show()


        
    resumeDataSet['cleaned_resume'] = resumeDataSet.Resume.apply(lambda x: cleanResume(x))

    #nltk.download('stopwords')
    #nltk.download('punkt')
    print ("Displaying the distinct categories of resume and the number of records belonging to each category -")
    print (resumeDataSet['Category'].value_counts())

    oneSetOfStopWords = set(stopwords.words('english')+['``',"''"])
    totalWords =[]
    Sentences = resumeDataSet['Resume'].values
    #Sentences = resumeDataSet['content'].values
    cleanedSentences = ""
    for i in range(0,160):
        cleanedText = cleanResume(Sentences[i])
        cleanedSentences += cleanedText
        requiredWords = nltk.word_tokenize(cleanedText)
        for word in requiredWords:
            if word not in oneSetOfStopWords and word not in string.punctuation:
                totalWords.append(word)
        
    wordfreqdist = nltk.FreqDist(totalWords)
    mostcommon = wordfreqdist.most_common(50)
    #print(mostcommon)

    #wc = WordCloud().generate(cleanedSentences)
    #plt.figure(figsize=(15,15))
    #plt.imshow(wc, interpolation='bilinear')
    #plt.axis("off")
    #plt.show()

    

    var_mod = ['Category']
    #var_mod = ['SKILLS']
    ##hier geÃ¤ndert
    le = LabelEncoder()
    for i in var_mod:
        resumeDataSet[i] = le.fit_transform(resumeDataSet[i])



    requiredText = resumeDataSet['cleaned_resume'].values
    requiredTarget = resumeDataSet['Category'].values
    



    word_vectorizer = TfidfVectorizer(
        sublinear_tf=True,
        stop_words='english',
        max_features=numbers_of_rows)
    word_vectorizer.fit(requiredText)
    WordFeatures = word_vectorizer.transform(requiredText)

    #print ("Feature completed .....")

    X_train,X_test,y_train,y_test = train_test_split(WordFeatures,requiredTarget,random_state=0, test_size=0.2)
    #print(X_train.shape)
    #print(X_test.shape)

    clf = OneVsRestClassifier(KNeighborsClassifier())
    clf.fit(X_train, y_train)
    prediction = clf.predict(X_test)
    #print(X_test)
    #print('Accuracy of KNeighbors Classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))
    #print('Accuracy of KNeighbors Classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))

    print("\n Classification report for classifier %s:\n%s\n" % (clf, metrics.classification_report(y_test, prediction)))

    realPrediction = clf.predict(X_train[5])

    resumeDataSet = pd.read_csv('UpdatedResumeDataSet.csv' ,encoding='utf-8')
    resumeDataSet['cleaned_resume'] = ''
    testDataSet = pd.read_json(jsonfile)
    #testDataSet = pd.read_json('test.json')
    testDataSet['cleaned_resume']=''
    resumeDataSet.head()
    testDataSet.head()









    """####Pre-Process the Dataset"""

    targetCounts = resumeDataSet['Category'].value_counts()
    targetLabels  = resumeDataSet['Category'].unique()
        
    resumeDataSet['cleaned_resume'] = resumeDataSet.Resume.apply(lambda x: cleanString(x))
    testDataSet['cleaned_resume'] = testDataSet.Resume.apply(lambda x: cleanString(x))

    var_mod = ['Category']
    le = LabelEncoder()
    for i in var_mod:
        resumeDataSet[i] = le.fit_transform(resumeDataSet[i])

    requiredText = resumeDataSet['cleaned_resume'].values
    requiredText2 = testDataSet['cleaned_resume'].values

    WordFeatures = vectorize(requiredText)
    WordFeatures2 = vectorize(requiredText2)
    #why 1500?
    WordFeatures2.resize(1,numbers_of_rows)
    #print(WordFeatures2.shape)
    #print(WordFeatures.shape)

    """####Train the model"""

    X_train,X_test,y_train,y_test = train_test_split(WordFeatures,requiredTarget,random_state=0, test_size=0.2)

    clf = OneVsRestClassifier(KNeighborsClassifier())
    clf.fit(X_train, y_train)

    """####Predict

    first we use the test data to validate our model
    """

    prediction = clf.predict(X_test)

    """and then we can use a singular x value and let the model predict the label for us"""
    

    print('Calculating the metrics...')
    print("Accuracy:{}".format(accuracy_score(y_test, prediction)))
    #print("recision_score:{}".format(precision_score(y_test, prediction, average='macro')))
    #print("recall_score:{}".format(precision_score(y_test, prediction, average='macro')))
    

    #print(classification_report(y_test, prediction))
    #print(confusion_matrix(y_test, prediction))
    #roc_auc_score(y_test, prediction)
    realPrediction = clf.predict(WordFeatures2)
    return(targetLabels[realPrediction[0]])



def get_job():
    keyword = run()


    cj  =  CareerjetAPIClient("en_GB")

    result_json = cj.search({
                            'location'    : 'berlin',
                            'keywords'    : keyword,
                            'affid'       : '213e213hd12344552',
                            'user_ip'     : '11.22.33.44',
                            'url'         : 'http://www.example.com/jobsearch?q=python&l=berlin',
                            'user_agent'  : 'Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Firefox/31.0'
                        })
    result_json["category"] = keyword
    return result_json
        
    

       

def cleaning_json(jsonfile="testResume.json"):
        # Opening JSON file
    f = open(jsonfile)
    

    # returns JSON object as 
    # a dictionary
    data = json.load(f)
    
    # Iterating through the json
    # list
    data[0]['Resume'] = ""

    with open(jsonfile, 'w') as f:
        json.dump(data, f)
    
    # Closing file
    f.close()




def correct_result(pdf_file, category,jsonfile="testResume.json" ):
    #s = '"{}"'.format(str(pdf_to_json(pdf_file,jsonfile)))
    List = [category, str(pdf_to_json(pdf_file,jsonfile))]
    with open('UpdatedResumeDataSet.csv', 'a') as f_object:
        writer_object = writer(f_object)
        writer_object.writerow(List)
        f_object.close()

